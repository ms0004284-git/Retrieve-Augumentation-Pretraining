{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5-DMLM(train-sciq-passage-level) Text2Text Generation on Sciq\n",
    "使用 Sciq dataset訓練 T5 Distractor Generation<br>\n",
    "直接使用 trainer 訓練 <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 25 20:34:50 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX               On  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 40%   38C    P8              36W / 280W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX               On  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 41%   36C    P8              31W / 280W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test on T5 with T5\"\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = project_name\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset sciq (/user_data/.cache/huggingface/datasets/sciq/default/0.1.0/50e5c6e3795b55463819d399ec417bfd4c3c621105e00295ddb5f3633d708493)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bad768962d04b96bf6c70ee271aff32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sciq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What type of organism is commonly used in preparation of foods such as cheese and yogurt?',\n",
       " 'distractor3': 'viruses',\n",
       " 'distractor1': 'protozoa',\n",
       " 'distractor2': 'gymnosperms',\n",
       " 'correct_answer': 'mesophilic organisms',\n",
       " 'support': 'Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "valid = dataset['validation']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(train)\n",
    "test = list(test)\n",
    "valid = list(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11679, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train = shuffle(train, random_state=777)\n",
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "減少training data 對結果的影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In the presence of oxygen, hydrogen can interact to make what?',\n",
       " 'distractor3': 'acid',\n",
       " 'distractor1': 'carbon',\n",
       " 'distractor2': 'helium',\n",
       " 'correct_answer': 'water',\n",
       " 'support': 'A pile of leaves slowly rots in the backyard. In the presence of oxygen, hydrogen can interact to make water. Gold can be stretched into very thin wires.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    answers = []\n",
    "    for d in data:\n",
    "        sentence = d['question']\n",
    "        distractors = [d['distractor1'], d['distractor2'], d['distractor3']]\n",
    "        answer = d['correct_answer']\n",
    "        \n",
    "        # 避免dataset的label有空白\n",
    "        distractors = [dis.strip() for dis in distractors]\n",
    "        \n",
    "        sentences.append(sentence)\n",
    "        labels.append('_ of distractors are ' + ', '.join(distractors))\n",
    "        answers.append(answer)\n",
    "        \n",
    "    return sentences, answers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent, train_answer, train_label = processData(train)\n",
    "valid_sent, valid_answer, valid_label = processData(valid)\n",
    "test_sent, test_answer, test_label = processData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ of distractors are antioxidants, Oxygen, residues\n"
     ]
    }
   ],
   "source": [
    "print(test_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ of distractors are invisible light, sunlight, ultraviolet light\n"
     ]
    }
   ],
   "source": [
    "for l in test_label:\n",
    "    if 'ultraviolet light' in l:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sent), len(train_answer), len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the presence of oxygen, hydrogen can interact to make what?\n",
      "water\n",
      "_ of distractors are carbon, helium, acid\n",
      "\n",
      "What type of diagnosis happens before a baby is born?\n",
      "prenatal\n",
      "_ of distractors are maternal, fetal, postnatal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    print(train_sent[idx])\n",
    "    print(train_answer[idx])\n",
    "    print(train_label[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_sent, train_answer,truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_sent, valid_answer,truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sent, test_answer,truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86, 8, 3053, 13, 11035, 6, 20913, 54, 6815, 12, 143, 125, 58, 1, 387, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the presence of oxygen, hydrogen can interact to make what?</s> water</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(encodings, distractors):\n",
    "    \n",
    "    distractors_encodings = tokenizer(distractors, padding=True)\n",
    "    labels = []\n",
    "    for i in range(len(distractors_encodings.input_ids)):\n",
    "        labels.append(distractors_encodings.input_ids[i])\n",
    "    \n",
    "    encodings[\"labels\"] = labels\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = add_labels(train_encodings, train_label)\n",
    "valid_encodings = add_labels(valid_encodings, valid_label)\n",
    "test_encodings = add_labels(test_encodings, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SciqDataset(train_encodings)\n",
    "valid_dataset = SciqDataset(valid_encodings)\n",
    "test_dataset = SciqDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 1000, 1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results-1\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=50,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"P@1\",\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps = 1,\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_PROJECT\") else \"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # store all article\n",
    "    predicted = []\n",
    "    true_label = []\n",
    "    \n",
    "    for k in range(len(decoded_labels)):\n",
    "        pred = decoded_preds[k]\n",
    "        label = decoded_labels[k]\n",
    "\n",
    "        pred_list = pred.split(', ')\n",
    "        label_list = label.split(', ')\n",
    "        \n",
    "        pred_list[0] = pred_list[0].split('are ')[-1]\n",
    "        label_list[0] = label_list[0].split('are ')[-1]\n",
    "\n",
    "        predicted.append(pred_list)\n",
    "        true_label.append(label_list)\n",
    "\n",
    "    # evaluation metrics\n",
    "    p1 = 0\n",
    "    p3 = 0\n",
    "    r3 = 0\n",
    "    f3 = 0\n",
    "    for idx in range(len(true_label)):\n",
    "        distractors = predicted[idx]\n",
    "        labels = true_label[idx]\n",
    "\n",
    "        act_set = set(labels)\n",
    "        pred1_set = set(distractors[:1])\n",
    "        pred3_set = set(distractors[:3])\n",
    "\n",
    "        p_1 = len(act_set & pred1_set) / float(1)\n",
    "        p_3 = len(act_set & pred3_set) / float(3)\n",
    "        r_3 = len(act_set & pred3_set) / float(len(act_set))\n",
    "\n",
    "        if p_3 == 0 and r_3 == 0:\n",
    "            f1_3 = 0\n",
    "        else:\n",
    "            f1_3 = 2 * (p_3 * r_3 / (p_3 + r_3))\n",
    "\n",
    "        p1+=p_1\n",
    "        p3+=p_3\n",
    "        r3+=r_3\n",
    "        f3+=f1_3\n",
    "\n",
    "    avg_p1 = p1 / len(true_label)\n",
    "    avg_p3 = p3 / len(true_label)\n",
    "    avg_r3 = r3 / len(true_label)\n",
    "    avg_f3 = f3 / len(true_label)\n",
    "\n",
    "    result = {'P@1': avg_p1,\n",
    "              'P@3': avg_p3,\n",
    "              'R@3': avg_r3,\n",
    "              'F1@3': avg_f3}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 5000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mms0004284\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_5000/wandb/run-20230825_203513-d834hiuh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ms0004284/test%20on%20T5%20with%20T5/runs/d834hiuh\" target=\"_blank\">./results-1</a></strong> to <a href=\"https://wandb.ai/ms0004284/test%20on%20T5%20with%20T5\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7850' max='7850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7850/7850 1:44:26, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>R@3</th>\n",
       "      <th>F1@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.539985</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.048333</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.526269</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.052667</td>\n",
       "      <td>0.052833</td>\n",
       "      <td>0.052733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.520889</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>0.059667</td>\n",
       "      <td>0.059622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.578700</td>\n",
       "      <td>0.516253</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.068333</td>\n",
       "      <td>0.068333</td>\n",
       "      <td>0.068244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.578700</td>\n",
       "      <td>0.516119</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.074167</td>\n",
       "      <td>0.074022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.578700</td>\n",
       "      <td>0.521500</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.084667</td>\n",
       "      <td>0.084833</td>\n",
       "      <td>0.084644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.521129</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.079333</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.079311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.525852</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.089333</td>\n",
       "      <td>0.089833</td>\n",
       "      <td>0.089489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.354800</td>\n",
       "      <td>0.531108</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.096333</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.096422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.538290</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.099667</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.099756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.542889</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.101333</td>\n",
       "      <td>0.101044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.550999</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.101333</td>\n",
       "      <td>0.101833</td>\n",
       "      <td>0.101489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.555121</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.102667</td>\n",
       "      <td>0.103167</td>\n",
       "      <td>0.102822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.561450</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.102667</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.102711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.572949</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.108167</td>\n",
       "      <td>0.107978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.574250</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.109333</td>\n",
       "      <td>0.109667</td>\n",
       "      <td>0.109378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.582327</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.108833</td>\n",
       "      <td>0.108489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.588931</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.114156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.253400</td>\n",
       "      <td>0.595326</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.114333</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>0.114489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.603579</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.115667</td>\n",
       "      <td>0.116167</td>\n",
       "      <td>0.115822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.610549</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.117667</td>\n",
       "      <td>0.118167</td>\n",
       "      <td>0.117822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.228700</td>\n",
       "      <td>0.614494</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.116156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.621755</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.123143</td>\n",
       "      <td>0.122911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.623155</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.122333</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.122311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.630932</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.121333</td>\n",
       "      <td>0.121667</td>\n",
       "      <td>0.121422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.637027</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.123476</td>\n",
       "      <td>0.123289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.644229</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.124156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.649733</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.128500</td>\n",
       "      <td>0.128156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.173100</td>\n",
       "      <td>0.655367</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>0.122156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.173100</td>\n",
       "      <td>0.657846</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.123667</td>\n",
       "      <td>0.124167</td>\n",
       "      <td>0.123822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.173100</td>\n",
       "      <td>0.667695</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.124333</td>\n",
       "      <td>0.124833</td>\n",
       "      <td>0.124489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.673626</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.123156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.675576</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.127333</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>0.127489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.677969</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.125667</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.125711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.158800</td>\n",
       "      <td>0.679751</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.126333</td>\n",
       "      <td>0.126044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.688355</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.124156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.694254</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.127667</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.127711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.694263</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.125156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.698206</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.127333</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>0.127489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.700998</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.127000</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.127156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.703430</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.130156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.706578</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.131333</td>\n",
       "      <td>0.131833</td>\n",
       "      <td>0.131489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.709709</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.129333</td>\n",
       "      <td>0.129833</td>\n",
       "      <td>0.129489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.709864</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.127333</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>0.127489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.713125</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.132310</td>\n",
       "      <td>0.132022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.714815</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.129333</td>\n",
       "      <td>0.129833</td>\n",
       "      <td>0.129489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.124400</td>\n",
       "      <td>0.714561</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.128667</td>\n",
       "      <td>0.129167</td>\n",
       "      <td>0.128822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.714245</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.129333</td>\n",
       "      <td>0.129833</td>\n",
       "      <td>0.129489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.715166</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.129156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>0.715478</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.128667</td>\n",
       "      <td>0.129167</td>\n",
       "      <td>0.128822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-157\n",
      "Configuration saved in ./results-1/checkpoint-157/config.json\n",
      "Model weights saved in ./results-1/checkpoint-157/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-157/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-157/special_tokens_map.json\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-314\n",
      "Configuration saved in ./results-1/checkpoint-314/config.json\n",
      "Model weights saved in ./results-1/checkpoint-314/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-314/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-314/special_tokens_map.json\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-471\n",
      "Configuration saved in ./results-1/checkpoint-471/config.json\n",
      "Model weights saved in ./results-1/checkpoint-471/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-471/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-471/special_tokens_map.json\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-628\n",
      "Configuration saved in ./results-1/checkpoint-628/config.json\n",
      "Model weights saved in ./results-1/checkpoint-628/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-628/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-628/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-157] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-785\n",
      "Configuration saved in ./results-1/checkpoint-785/config.json\n",
      "Model weights saved in ./results-1/checkpoint-785/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-785/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-785/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-314] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-942\n",
      "Configuration saved in ./results-1/checkpoint-942/config.json\n",
      "Model weights saved in ./results-1/checkpoint-942/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-942/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-942/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-471] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1099\n",
      "Configuration saved in ./results-1/checkpoint-1099/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1099/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1099/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1099/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-628] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1256\n",
      "Configuration saved in ./results-1/checkpoint-1256/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1256/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1256/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1256/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-785] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1413\n",
      "Configuration saved in ./results-1/checkpoint-1413/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1413/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1413/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1413/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-942] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1570\n",
      "Configuration saved in ./results-1/checkpoint-1570/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1570/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1570/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1570/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1099] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1727\n",
      "Configuration saved in ./results-1/checkpoint-1727/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1727/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1727/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1727/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1256] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1884\n",
      "Configuration saved in ./results-1/checkpoint-1884/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1884/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1884/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1884/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1413] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2041\n",
      "Configuration saved in ./results-1/checkpoint-2041/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2041/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2041/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2041/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1570] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2198\n",
      "Configuration saved in ./results-1/checkpoint-2198/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2198/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2198/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2198/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1727] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2355\n",
      "Configuration saved in ./results-1/checkpoint-2355/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2355/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2355/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2355/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2041] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2512\n",
      "Configuration saved in ./results-1/checkpoint-2512/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2512/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2512/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2512/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2198] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2669\n",
      "Configuration saved in ./results-1/checkpoint-2669/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2669/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2669/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2669/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2355] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2826\n",
      "Configuration saved in ./results-1/checkpoint-2826/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2826/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2826/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2826/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1884] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2983\n",
      "Configuration saved in ./results-1/checkpoint-2983/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2983/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2983/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2983/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2512] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-3140\n",
      "Configuration saved in ./results-1/checkpoint-3140/config.json\n",
      "Model weights saved in ./results-1/checkpoint-3140/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-3140/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-3140/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2669] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-3297\n",
      "Configuration saved in ./results-1/checkpoint-3297/config.json\n",
      "Model weights saved in ./results-1/checkpoint-3297/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-3297/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-3297/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2983] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-3454\n",
      "Configuration saved in ./results-1/checkpoint-3454/config.json\n",
      "Model weights saved in ./results-1/checkpoint-3454/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-3454/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-3454/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-3140] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-3611\n",
      "Configuration saved in ./results-1/checkpoint-3611/config.json\n",
      "Model weights saved in ./results-1/checkpoint-3611/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-3611/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-3611/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2826] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-3768\n",
      "Configuration saved in ./results-1/checkpoint-3768/config.json\n",
      "Model weights saved in ./results-1/checkpoint-3768/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-3768/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-3768/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-3297] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-3925\n",
      "Configuration saved in ./results-1/checkpoint-3925/config.json\n",
      "Model weights saved in ./results-1/checkpoint-3925/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-3925/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-3925/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-3454] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4082\n",
      "Configuration saved in ./results-1/checkpoint-4082/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4082/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4082/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4082/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-3768] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4239\n",
      "Configuration saved in ./results-1/checkpoint-4239/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4239/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4239/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4239/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-3925] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4396\n",
      "Configuration saved in ./results-1/checkpoint-4396/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4396/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4396/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4396/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4082] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4553\n",
      "Configuration saved in ./results-1/checkpoint-4553/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4553/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4553/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4553/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4239] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4710\n",
      "Configuration saved in ./results-1/checkpoint-4710/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4710/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4710/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4710/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4396] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4867\n",
      "Configuration saved in ./results-1/checkpoint-4867/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4867/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4867/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4867/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4553] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5024\n",
      "Configuration saved in ./results-1/checkpoint-5024/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5024/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5024/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5024/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4710] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5181\n",
      "Configuration saved in ./results-1/checkpoint-5181/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5181/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5181/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5181/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4867] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5338\n",
      "Configuration saved in ./results-1/checkpoint-5338/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5338/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5338/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5338/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-3611] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5495\n",
      "Configuration saved in ./results-1/checkpoint-5495/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5495/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5495/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5495/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5024] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5652\n",
      "Configuration saved in ./results-1/checkpoint-5652/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5652/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5652/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5652/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5181] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5809\n",
      "Configuration saved in ./results-1/checkpoint-5809/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5809/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5809/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5809/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5495] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5966\n",
      "Configuration saved in ./results-1/checkpoint-5966/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5966/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5966/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5966/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5652] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6123\n",
      "Configuration saved in ./results-1/checkpoint-6123/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6123/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6123/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6123/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5809] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6280\n",
      "Configuration saved in ./results-1/checkpoint-6280/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6280/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6280/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6280/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5966] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6437\n",
      "Configuration saved in ./results-1/checkpoint-6437/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6437/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6437/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6437/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6123] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6594\n",
      "Configuration saved in ./results-1/checkpoint-6594/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6594/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6594/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6594/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6280] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6751\n",
      "Configuration saved in ./results-1/checkpoint-6751/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6751/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6751/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6751/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6437] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6908\n",
      "Configuration saved in ./results-1/checkpoint-6908/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6908/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6908/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6908/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6594] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-7065\n",
      "Configuration saved in ./results-1/checkpoint-7065/config.json\n",
      "Model weights saved in ./results-1/checkpoint-7065/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-7065/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-7065/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6751] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-7222\n",
      "Configuration saved in ./results-1/checkpoint-7222/config.json\n",
      "Model weights saved in ./results-1/checkpoint-7222/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-7222/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-7222/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6908] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-7379\n",
      "Configuration saved in ./results-1/checkpoint-7379/config.json\n",
      "Model weights saved in ./results-1/checkpoint-7379/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-7379/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-7379/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-7065] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-7536\n",
      "Configuration saved in ./results-1/checkpoint-7536/config.json\n",
      "Model weights saved in ./results-1/checkpoint-7536/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-7536/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-7536/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-7222] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-7693\n",
      "Configuration saved in ./results-1/checkpoint-7693/config.json\n",
      "Model weights saved in ./results-1/checkpoint-7693/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-7693/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-7693/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-7379] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-7850\n",
      "Configuration saved in ./results-1/checkpoint-7850/config.json\n",
      "Model weights saved in ./results-1/checkpoint-7850/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-7850/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-7850/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-7536] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results-1/checkpoint-5338 (score: 0.193).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7850, training_loss=0.22188263716971038, metrics={'train_runtime': 6271.632, 'train_samples_per_second': 39.862, 'train_steps_per_second': 1.252, 'total_flos': 3.300504192e+16, 'train_loss': 0.22188263716971038, 'epoch': 50.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6779685616493225,\n",
       " 'eval_P@1': 0.193,\n",
       " 'eval_P@3': 0.1256666666666664,\n",
       " 'eval_R@3': 0.12599999999999972,\n",
       " 'eval_F1@3': 0.12571111111111083,\n",
       " 'eval_runtime': 30.6466,\n",
       " 'eval_samples_per_second': 32.63,\n",
       " 'eval_steps_per_second': 1.044,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_5000/model/t5-base-text2text-sciq-off-shelf-5000\n",
      "Configuration saved in /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_5000/model/t5-base-text2text-sciq-off-shelf-5000/config.json\n",
      "Model weights saved in /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_5000/model/t5-base-text2text-sciq-off-shelf-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_5000/model/t5-base-text2text-sciq-off-shelf-5000/tokenizer_config.json\n",
      "Special tokens file saved in /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_5000/model/t5-base-text2text-sciq-off-shelf-5000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('/user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_2000/model/t5-base-text2text-sciq-off-shelf-2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.6779685616493225,\n",
       " 'test_P@1': 0.193,\n",
       " 'test_P@3': 0.1256666666666664,\n",
       " 'test_R@3': 0.12599999999999972,\n",
       " 'test_F1@3': 0.12571111111111083,\n",
       " 'test_runtime': 30.7869,\n",
       " 'test_samples_per_second': 32.481,\n",
       " 'test_steps_per_second': 1.039}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(valid_dataset)\n",
    "print('valid: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.8025060892105103,\n",
       " 'test_P@1': 0.195,\n",
       " 'test_P@3': 0.1366666666666666,\n",
       " 'test_R@3': 0.1368888888888888,\n",
       " 'test_F1@3': 0.13659999999999992,\n",
       " 'test_runtime': 27.8392,\n",
       " 'test_samples_per_second': 35.921,\n",
       " 'test_steps_per_second': 1.149}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(test_dataset)\n",
    "print('test: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_json(data, path):\n",
    "    \n",
    "    jsonString = json.dumps(data)\n",
    "    jsonFile = open(path, \"w\")\n",
    "    jsonFile.write(jsonString)\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, predictions, labels, file_name):\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # store all article\n",
    "    predicted = []\n",
    "    true_label = []\n",
    "    \n",
    "    for k in range(len(decoded_labels)):\n",
    "        pred = decoded_preds[k]\n",
    "        label = decoded_labels[k]\n",
    "\n",
    "        pred_list = pred.split(', ')\n",
    "        label_list = label.split(', ')\n",
    "        \n",
    "        pred_list[0] = pred_list[0].split('are ')[-1]\n",
    "        label_list[0] = label_list[0].split('are ')[-1]\n",
    "\n",
    "        predicted.append(pred_list)\n",
    "        true_label.append(label_list)\n",
    "    \n",
    "    \n",
    "    # evaluation metrics\n",
    "    for idx in range(len(true_label)):\n",
    "        distractors = predicted[idx]\n",
    "        labels = true_label[idx]\n",
    "        \n",
    "        data[idx]['pred_distractors'] = distractors\n",
    "\n",
    "        act_set = set(labels)\n",
    "        pred1_set = set(distractors[:1])\n",
    "        pred3_set = set(distractors[:3])\n",
    "\n",
    "        p_1 = len(act_set & pred1_set) / float(1)\n",
    "        p_3 = len(act_set & pred3_set) / float(3)\n",
    "        r_3 = len(act_set & pred3_set) / float(len(act_set))\n",
    "\n",
    "        if p_3 == 0 and r_3 == 0:\n",
    "            f1_3 = 0\n",
    "        else:\n",
    "            f1_3 = 2 * (p_3 * r_3 / (p_3 + r_3))\n",
    "            \n",
    "        data[idx]['metric'] = {'P@1': p_1, 'P@3': p_3, 'R@3': r_3, 'F1@3': f1_3}\n",
    "        \n",
    "    write_json(data, file_name)\n",
    "    print(file_name + ' is saved :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/CTG/test_result/sciq_test_t5_text2text_off_shelf_5000.json is saved :)\n"
     ]
    }
   ],
   "source": [
    "save_data(test, predictions, labels, '/user_data/CTG/test_result/sciq_test_t5_text2text_off_shelf_2000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/added_tokens.json. We won't load it.\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/spiece.model\n",
      "loading file None\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/special_tokens_map.json\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/tokenizer_config.json\n",
      "loading configuration file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n",
      "loading weights file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=50,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"P@1\",\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.8240611553192139,\n",
       " 'test_P@1': 0.185,\n",
       " 'test_P@3': 0.11733333333333297,\n",
       " 'test_R@3': 0.11783333333333297,\n",
       " 'test_F1@3': 0.11753333333333298,\n",
       " 'test_runtime': 25.8663,\n",
       " 'test_samples_per_second': 38.66,\n",
       " 'test_steps_per_second': 0.309}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(valid_dataset)\n",
    "print('valid: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.0030803680419922,\n",
       " 'test_P@1': 0.198,\n",
       " 'test_P@3': 0.1173333333333331,\n",
       " 'test_R@3': 0.11745238095238072,\n",
       " 'test_F1@3': 0.11726666666666644,\n",
       " 'test_runtime': 23.4762,\n",
       " 'test_samples_per_second': 42.596,\n",
       " 'test_steps_per_second': 0.341}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions, test_labels, test_metrics = trainer.predict(test_dataset)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_data('/user_data/CTG/test_result/sciq_test_t5_text2text_pretrain_on_sciq_training_set_passage_level_dtt_retrieve_8000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Compounds that are capable of accepting electrons, such as o 2 or f2, are called what?\n",
      "answer: oxidants\n",
      "distractors: ['antioxidants', 'Oxygen', 'residues']\n",
      "predict: ['oxides', 'particles', 'anions']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Which type of tree is dominant in temperate forests?\n",
      "answer: deciduous\n",
      "distractors: ['vines', 'fungus', 'shrubs']\n",
      "predict: ['perennial', 'fibrous', 'coniferous']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Only about one percent of plants have lost what ability, turning them into consumers and even predators, instead of producers?\n",
      "answer: photosynthesis\n",
      "distractors: ['flowering', 'rooting', 'growth']\n",
      "predict: ['glycolysis', 'atherosclerosis', 'absorption']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Presence of a cell wall, large central vacuole, and organelles called plastids distinguish what type of cell?\n",
      "answer: plant\n",
      "distractors: ['animal', 'reproductive', 'heterotroph']\n",
      "predict: ['bird', 'animal', 'insect']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.3333333333333333, 'R@3': 0.3333333333333333, 'F1@3': 0.3333333333333333}\n",
      "\n",
      "question: Digestion of proteins begins with acids in what organ?\n",
      "answer: stomach\n",
      "distractors: ['colon', 'liver', 'brain']\n",
      "predict: ['kidneys', 'skin', 'respiratory']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: In what type of animals may a body cavity be present or absent?\n",
      "answer: triploblastic\n",
      "distractors: ['vertebrate', 'bicellular', 'nonvascular']\n",
      "predict: ['spongin', 'collagen', 'scleroprotein']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Rupture can cause fragments of what to travel via the bloodstream and become lodged in other arteries?\n",
      "answer: plaque\n",
      "distractors: ['enamel', 'red blood cells', 'white blood cells']\n",
      "predict: ['triglycerides', 'fat', 'marble']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What record shows that dinosaurs originated 200-250 million years ago?\n",
      "answer: fossil record\n",
      "distractors: ['ancient record', 'biological record', 'species record']\n",
      "predict: ['fossil magnitude', 'coal record', 'fuel cycle']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Ibuprofen and albuterol are examples of drugs whose _________ have different effects.\n",
      "answer: enantiomers\n",
      "distractors: ['nanoparticles', 'misnomers', 'analogous']\n",
      "predict: ['amines', 'ketones', 'acids']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What is another name for the vertebral column?\n",
      "answer: backbone\n",
      "distractors: ['nerve column', 'pillar', 'brain stem']\n",
      "predict: ['rib cage', 'umbilical cord', 'brain stem']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.3333333333333333, 'R@3': 0.3333333333333333, 'F1@3': 0.3333333333333333}\n",
      "\n",
      "question: When equal amounts of a strong acid such as hydrochloric acid are mixed with a strong base such as sodium hydroxide, the result is what kind of solution?\n",
      "answer: a neutral one\n",
      "distractors: ['a economical one', 'a lateral one', 'a thermodynamic one']\n",
      "predict: ['a neutral one', 'a positive one', 'a neutral']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What is required to move or change matter from one state to another?\n",
      "answer: energy\n",
      "distractors: ['food', 'gravity', 'evolution']\n",
      "predict: ['fuel', 'hydrogen', 'power']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What unit of measurement is defined to be the number of atoms in 12g of carbon-12?\n",
      "answer: one mole\n",
      "distractors: ['one quark', 'one joule', 'one ohm']\n",
      "predict: ['two mole', 'four electrons', 'one nucleus']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Movements in the mantle cause the plates to move over time in a process called what?\n",
      "answer: continental drift\n",
      "distractors: ['continental expansion', 'continental shift', 'boundary drift']\n",
      "predict: ['continental drift', 'continental stratosphere', 'continental solution']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: With wavelengths from 400-700 nm, what kind of light represents only a very small portion of the spectrum?\n",
      "answer: visible light\n",
      "distractors: ['invisible light', 'sunlight', 'ultraviolet light']\n",
      "predict: ['ultraviolet light', 'apparent light', 'infrared light']\n",
      "metric: {'P@1': 1.0, 'P@3': 0.3333333333333333, 'R@3': 0.3333333333333333, 'F1@3': 0.3333333333333333}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 100, 7):\n",
    "    example = test[i]\n",
    "    sentence = example['question']\n",
    "    answer = example['correct_answer']\n",
    "    distractors = [example['distractor1'], example['distractor2'], example['distractor3']]\n",
    "    pred_distractors = example['pred_distractors']\n",
    "    metric = example['metric']\n",
    "    \n",
    "    print('question:', sentence.replace('**blank**', '_'))\n",
    "    print('answer:', answer)\n",
    "    print('distractors:', distractors)\n",
    "    print('predict:', pred_distractors)\n",
    "    print('metric:', metric)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
