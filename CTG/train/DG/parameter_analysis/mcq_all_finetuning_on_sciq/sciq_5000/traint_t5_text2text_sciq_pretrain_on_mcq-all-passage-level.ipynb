{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5-DMLM(train-sciq-passage-level) Text2Text Generation on Sciq\n",
    "使用 Sciq dataset訓練 T5 Distractor Generation<br>\n",
    "直接使用 trainer 訓練 <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug 25 16:11:47 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA TITAN RTX               On  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 41%   39C    P8              35W / 280W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX               On  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 41%   36C    P8              32W / 280W |      1MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test on T5 with T5\"\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = project_name\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset sciq (/user_data/.cache/huggingface/datasets/sciq/default/0.1.0/50e5c6e3795b55463819d399ec417bfd4c3c621105e00295ddb5f3633d708493)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a105f3a6af0464e97fea2c38815dfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"sciq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 11679\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What type of organism is commonly used in preparation of foods such as cheese and yogurt?',\n",
       " 'distractor3': 'viruses',\n",
       " 'distractor1': 'protozoa',\n",
       " 'distractor2': 'gymnosperms',\n",
       " 'correct_answer': 'mesophilic organisms',\n",
       " 'support': 'Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "valid = dataset['validation']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(train)\n",
    "test = list(test)\n",
    "valid = list(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11679, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train = shuffle(train, random_state=777)\n",
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "減少training data 對結果的影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1000, 1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'In the presence of oxygen, hydrogen can interact to make what?',\n",
       " 'distractor3': 'acid',\n",
       " 'distractor1': 'carbon',\n",
       " 'distractor2': 'helium',\n",
       " 'correct_answer': 'water',\n",
       " 'support': 'A pile of leaves slowly rots in the backyard. In the presence of oxygen, hydrogen can interact to make water. Gold can be stretched into very thin wires.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    answers = []\n",
    "    for d in data:\n",
    "        sentence = d['question']\n",
    "        distractors = [d['distractor1'], d['distractor2'], d['distractor3']]\n",
    "        answer = d['correct_answer']\n",
    "        \n",
    "        # 避免dataset的label有空白\n",
    "        distractors = [dis.strip() for dis in distractors]\n",
    "        \n",
    "        sentences.append(sentence)\n",
    "        labels.append('_ of distractors are ' + ', '.join(distractors))\n",
    "        answers.append(answer)\n",
    "        \n",
    "    return sentences, answers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent, train_answer, train_label = processData(train)\n",
    "valid_sent, valid_answer, valid_label = processData(valid)\n",
    "test_sent, test_answer, test_label = processData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ of distractors are antioxidants, Oxygen, residues\n"
     ]
    }
   ],
   "source": [
    "print(test_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ of distractors are invisible light, sunlight, ultraviolet light\n"
     ]
    }
   ],
   "source": [
    "for l in test_label:\n",
    "    if 'ultraviolet light' in l:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000, 8000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sent), len(train_answer), len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the presence of oxygen, hydrogen can interact to make what?\n",
      "water\n",
      "_ of distractors are carbon, helium, acid\n",
      "\n",
      "What type of diagnosis happens before a baby is born?\n",
      "prenatal\n",
      "_ of distractors are maternal, fetal, postnatal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    print(train_sent[idx])\n",
    "    print(train_answer[idx])\n",
    "    print(train_label[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_sent, train_answer,truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_sent, valid_answer,truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sent, test_answer,truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86, 8, 3053, 13, 11035, 6, 20913, 54, 6815, 12, 143, 125, 58, 1, 387, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the presence of oxygen, hydrogen can interact to make what?</s> water</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(encodings, distractors):\n",
    "    \n",
    "    distractors_encodings = tokenizer(distractors, padding=True)\n",
    "    labels = []\n",
    "    for i in range(len(distractors_encodings.input_ids)):\n",
    "        labels.append(distractors_encodings.input_ids[i])\n",
    "    \n",
    "    encodings[\"labels\"] = labels\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = add_labels(train_encodings, train_label)\n",
    "valid_encodings = add_labels(valid_encodings, valid_label)\n",
    "test_encodings = add_labels(test_encodings, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SciqDataset(train_encodings)\n",
    "valid_dataset = SciqDataset(valid_encodings)\n",
    "test_dataset = SciqDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 1000, 1000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = torch.nn.ModuleDict({\n",
    "    'model': model,\n",
    "})\n",
    "checkpoint = torch.load('/user_data/Cloze/dtt_mask_lm_model/mcq_all_passage_level/epoch=03-dev_loss=0.12.ckpt')\n",
    "model_dict.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results-0\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=50,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"P@1\",\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps = 1,\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_PROJECT\") else \"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # store all article\n",
    "    predicted = []\n",
    "    true_label = []\n",
    "    \n",
    "    for k in range(len(decoded_labels)):\n",
    "        pred = decoded_preds[k]\n",
    "        label = decoded_labels[k]\n",
    "\n",
    "        pred_list = pred.split(', ')\n",
    "        label_list = label.split(', ')\n",
    "        \n",
    "        pred_list[0] = pred_list[0].split('are ')[-1]\n",
    "        label_list[0] = label_list[0].split('are ')[-1]\n",
    "\n",
    "        predicted.append(pred_list)\n",
    "        true_label.append(label_list)\n",
    "\n",
    "    # evaluation metrics\n",
    "    p1 = 0\n",
    "    p3 = 0\n",
    "    r3 = 0\n",
    "    f3 = 0\n",
    "    for idx in range(len(true_label)):\n",
    "        distractors = predicted[idx]\n",
    "        labels = true_label[idx]\n",
    "\n",
    "        act_set = set(labels)\n",
    "        pred1_set = set(distractors[:1])\n",
    "        pred3_set = set(distractors[:3])\n",
    "\n",
    "        p_1 = len(act_set & pred1_set) / float(1)\n",
    "        p_3 = len(act_set & pred3_set) / float(3)\n",
    "        r_3 = len(act_set & pred3_set) / float(len(act_set))\n",
    "\n",
    "        if p_3 == 0 and r_3 == 0:\n",
    "            f1_3 = 0\n",
    "        else:\n",
    "            f1_3 = 2 * (p_3 * r_3 / (p_3 + r_3))\n",
    "\n",
    "        p1+=p_1\n",
    "        p3+=p_3\n",
    "        r3+=r_3\n",
    "        f3+=f1_3\n",
    "\n",
    "    avg_p1 = p1 / len(true_label)\n",
    "    avg_p3 = p3 / len(true_label)\n",
    "    avg_r3 = r3 / len(true_label)\n",
    "    avg_f3 = f3 / len(true_label)\n",
    "\n",
    "    result = {'P@1': avg_p1,\n",
    "              'P@3': avg_p3,\n",
    "              'R@3': avg_r3,\n",
    "              'F1@3': avg_f3}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 8000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mms0004284\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd36062a3c54a62b7bd34d23aecbb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668258416636186, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_8000/wandb/run-20230825_161210-3ieaij0n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ms0004284/test%20on%20T5%20with%20T5/runs/3ieaij0n\" target=\"_blank\">./results-0</a></strong> to <a href=\"https://wandb.ai/ms0004284/test%20on%20T5%20with%20T5\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 2:23:39, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>R@3</th>\n",
       "      <th>F1@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.525311</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.044333</td>\n",
       "      <td>0.044333</td>\n",
       "      <td>0.044289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.514264</td>\n",
       "      <td>0.116000</td>\n",
       "      <td>0.052333</td>\n",
       "      <td>0.052310</td>\n",
       "      <td>0.052267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.507920</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.059956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.505112</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.075956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.377000</td>\n",
       "      <td>0.504551</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.082667</td>\n",
       "      <td>0.082422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.506584</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.087667</td>\n",
       "      <td>0.087643</td>\n",
       "      <td>0.087511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.509594</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.095667</td>\n",
       "      <td>0.095810</td>\n",
       "      <td>0.095578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.510521</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.095333</td>\n",
       "      <td>0.095476</td>\n",
       "      <td>0.095244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.515437</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.097000</td>\n",
       "      <td>0.096976</td>\n",
       "      <td>0.096844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.301100</td>\n",
       "      <td>0.519799</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.107667</td>\n",
       "      <td>0.107976</td>\n",
       "      <td>0.107689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.301100</td>\n",
       "      <td>0.526174</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.102667</td>\n",
       "      <td>0.103167</td>\n",
       "      <td>0.102822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.531065</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.108333</td>\n",
       "      <td>0.108833</td>\n",
       "      <td>0.108489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.283100</td>\n",
       "      <td>0.535251</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.112333</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.112311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.539709</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.112667</td>\n",
       "      <td>0.113167</td>\n",
       "      <td>0.112822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.546598</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.119333</td>\n",
       "      <td>0.119833</td>\n",
       "      <td>0.119489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.550799</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.118333</td>\n",
       "      <td>0.118667</td>\n",
       "      <td>0.118378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.560004</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.123156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.563943</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.123667</td>\n",
       "      <td>0.123378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.567722</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125310</td>\n",
       "      <td>0.125022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.575667</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.128667</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.128711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.578688</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.130143</td>\n",
       "      <td>0.129911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>0.584796</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>0.131333</td>\n",
       "      <td>0.131333</td>\n",
       "      <td>0.131244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.213100</td>\n",
       "      <td>0.591399</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.130667</td>\n",
       "      <td>0.130667</td>\n",
       "      <td>0.130578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>0.592277</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.132167</td>\n",
       "      <td>0.132022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.202600</td>\n",
       "      <td>0.603551</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.132333</td>\n",
       "      <td>0.132833</td>\n",
       "      <td>0.132533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.607377</td>\n",
       "      <td>0.203000</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.131333</td>\n",
       "      <td>0.131089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.192100</td>\n",
       "      <td>0.612255</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.136333</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>0.136422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>0.613182</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.133156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.183300</td>\n",
       "      <td>0.620037</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.622667</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.133000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.626399</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.134333</td>\n",
       "      <td>0.134833</td>\n",
       "      <td>0.134489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>0.632779</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.136156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.167700</td>\n",
       "      <td>0.635652</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.136976</td>\n",
       "      <td>0.136844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.642259</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.133667</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.133756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.160900</td>\n",
       "      <td>0.643351</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>0.138089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.650643</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.135667</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.135756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.651392</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.137143</td>\n",
       "      <td>0.136911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.655465</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.140667</td>\n",
       "      <td>0.141167</td>\n",
       "      <td>0.140822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.149500</td>\n",
       "      <td>0.658879</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.137156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.660837</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.132667</td>\n",
       "      <td>0.133167</td>\n",
       "      <td>0.132822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.144600</td>\n",
       "      <td>0.665443</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.136667</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>0.136711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.667043</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.139333</td>\n",
       "      <td>0.139667</td>\n",
       "      <td>0.139378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.669571</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.136333</td>\n",
       "      <td>0.136044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.670657</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.133667</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.133711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.674195</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.135333</td>\n",
       "      <td>0.135044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.675150</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>0.138667</td>\n",
       "      <td>0.138378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>0.677011</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>0.138044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.678072</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.139333</td>\n",
       "      <td>0.139667</td>\n",
       "      <td>0.139378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.678466</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.138333</td>\n",
       "      <td>0.138044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>0.678671</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.137667</td>\n",
       "      <td>0.138000</td>\n",
       "      <td>0.137711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-250\n",
      "Configuration saved in ./results-0/checkpoint-250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-250/special_tokens_map.json\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-500\n",
      "Configuration saved in ./results-0/checkpoint-500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-500/special_tokens_map.json\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-750\n",
      "Configuration saved in ./results-0/checkpoint-750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-750/special_tokens_map.json\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-1000\n",
      "Configuration saved in ./results-0/checkpoint-1000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-1250\n",
      "Configuration saved in ./results-0/checkpoint-1250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-1250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-1250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-1250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-1500\n",
      "Configuration saved in ./results-0/checkpoint-1500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-1750\n",
      "Configuration saved in ./results-0/checkpoint-1750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-1750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-1750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-1750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-1000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-2000\n",
      "Configuration saved in ./results-0/checkpoint-2000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-1250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-2250\n",
      "Configuration saved in ./results-0/checkpoint-2250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-2250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-2250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-2250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-1500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-2500\n",
      "Configuration saved in ./results-0/checkpoint-2500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-1750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-2750\n",
      "Configuration saved in ./results-0/checkpoint-2750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-2750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-2750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-2750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-2000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-3000\n",
      "Configuration saved in ./results-0/checkpoint-3000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-2250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-3250\n",
      "Configuration saved in ./results-0/checkpoint-3250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-3250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-3250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-3250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-2500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-3500\n",
      "Configuration saved in ./results-0/checkpoint-3500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-2750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-3750\n",
      "Configuration saved in ./results-0/checkpoint-3750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-3750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-3750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-3750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-3250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-4000\n",
      "Configuration saved in ./results-0/checkpoint-4000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-3500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-4250\n",
      "Configuration saved in ./results-0/checkpoint-4250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-4250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-4250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-4250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-3000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-4500\n",
      "Configuration saved in ./results-0/checkpoint-4500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-3750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-4750\n",
      "Configuration saved in ./results-0/checkpoint-4750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-4750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-4750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-4750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-4000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-5000\n",
      "Configuration saved in ./results-0/checkpoint-5000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-4250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-5250\n",
      "Configuration saved in ./results-0/checkpoint-5250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-5250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-5250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-5250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-4500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-5500\n",
      "Configuration saved in ./results-0/checkpoint-5500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-5000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-5750\n",
      "Configuration saved in ./results-0/checkpoint-5750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-5750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-5750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-5750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-5250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-6000\n",
      "Configuration saved in ./results-0/checkpoint-6000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-5500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-6250\n",
      "Configuration saved in ./results-0/checkpoint-6250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-6250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-6250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-6250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-4750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-6500\n",
      "Configuration saved in ./results-0/checkpoint-6500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-5750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-6750\n",
      "Configuration saved in ./results-0/checkpoint-6750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-6750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-6750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-6750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-6000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-7000\n",
      "Configuration saved in ./results-0/checkpoint-7000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-6250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-7250\n",
      "Configuration saved in ./results-0/checkpoint-7250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-7250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-7250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-7250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-6500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-7500\n",
      "Configuration saved in ./results-0/checkpoint-7500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-7000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-7750\n",
      "Configuration saved in ./results-0/checkpoint-7750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-7750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-7750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-7750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-7250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-8000\n",
      "Configuration saved in ./results-0/checkpoint-8000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-7500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-8250\n",
      "Configuration saved in ./results-0/checkpoint-8250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-8250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-8250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-8250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-7750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-8500\n",
      "Configuration saved in ./results-0/checkpoint-8500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-8000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-8750\n",
      "Configuration saved in ./results-0/checkpoint-8750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-8750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-8750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-8750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-8250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-9000\n",
      "Configuration saved in ./results-0/checkpoint-9000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-8500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-9250\n",
      "Configuration saved in ./results-0/checkpoint-9250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-9250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-9250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-9250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-8750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-9500\n",
      "Configuration saved in ./results-0/checkpoint-9500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-9000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-9750\n",
      "Configuration saved in ./results-0/checkpoint-9750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-9750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-9750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-9750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-9250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-10000\n",
      "Configuration saved in ./results-0/checkpoint-10000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-9500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-10250\n",
      "Configuration saved in ./results-0/checkpoint-10250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-10250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-10250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-10250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-9750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-10500\n",
      "Configuration saved in ./results-0/checkpoint-10500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-10000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-10750\n",
      "Configuration saved in ./results-0/checkpoint-10750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-10750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-10750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-10750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-10250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-11000\n",
      "Configuration saved in ./results-0/checkpoint-11000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-10500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-11250\n",
      "Configuration saved in ./results-0/checkpoint-11250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-11250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-11250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-11250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-10750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-11500\n",
      "Configuration saved in ./results-0/checkpoint-11500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-11000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-11750\n",
      "Configuration saved in ./results-0/checkpoint-11750/config.json\n",
      "Model weights saved in ./results-0/checkpoint-11750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-11750/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-11750/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-11250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-12000\n",
      "Configuration saved in ./results-0/checkpoint-12000/config.json\n",
      "Model weights saved in ./results-0/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-11500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-12250\n",
      "Configuration saved in ./results-0/checkpoint-12250/config.json\n",
      "Model weights saved in ./results-0/checkpoint-12250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-12250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-12250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-11750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-0/checkpoint-12500\n",
      "Configuration saved in ./results-0/checkpoint-12500/config.json\n",
      "Model weights saved in ./results-0/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-0/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in ./results-0/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [results-0/checkpoint-12000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results-0/checkpoint-6750 (score: 0.215).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=0.2191597833251953, metrics={'train_runtime': 8626.3091, 'train_samples_per_second': 46.37, 'train_steps_per_second': 1.449, 'total_flos': 5.2808067072e+16, 'train_loss': 0.2191597833251953, 'epoch': 50.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 03:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6122552156448364,\n",
       " 'eval_P@1': 0.215,\n",
       " 'eval_P@3': 0.13633333333333328,\n",
       " 'eval_R@3': 0.13666666666666663,\n",
       " 'eval_F1@3': 0.1364222222222222,\n",
       " 'eval_runtime': 31.1138,\n",
       " 'eval_samples_per_second': 32.14,\n",
       " 'eval_steps_per_second': 1.028,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_8000/model/t5-base-text2text-sciq-pretrain-on-mcq-all-passage-level-8000\n",
      "Configuration saved in /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_8000/model/t5-base-text2text-sciq-pretrain-on-mcq-all-passage-level-8000/config.json\n",
      "Model weights saved in /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_8000/model/t5-base-text2text-sciq-pretrain-on-mcq-all-passage-level-8000/pytorch_model.bin\n",
      "tokenizer config file saved in /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_8000/model/t5-base-text2text-sciq-pretrain-on-mcq-all-passage-level-8000/tokenizer_config.json\n",
      "Special tokens file saved in /user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_8000/model/t5-base-text2text-sciq-pretrain-on-mcq-all-passage-level-8000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('/user_data/CTG/train/DG/parameter_analysis/mcq_all_finetuning_on_sciq/sciq_5000/model/t5-base-text2text-sciq-pretrain-on-mcq-all-passage-level-5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.6122552156448364,\n",
       " 'test_P@1': 0.215,\n",
       " 'test_P@3': 0.13633333333333328,\n",
       " 'test_R@3': 0.13666666666666663,\n",
       " 'test_F1@3': 0.1364222222222222,\n",
       " 'test_runtime': 30.4716,\n",
       " 'test_samples_per_second': 32.817,\n",
       " 'test_steps_per_second': 1.05}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(valid_dataset)\n",
    "print('valid: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.7293282747268677,\n",
       " 'test_P@1': 0.231,\n",
       " 'test_P@3': 0.15533333333333327,\n",
       " 'test_R@3': 0.15484126984126975,\n",
       " 'test_F1@3': 0.15486666666666662,\n",
       " 'test_runtime': 32.1349,\n",
       " 'test_samples_per_second': 31.119,\n",
       " 'test_steps_per_second': 0.996}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(test_dataset)\n",
    "print('test: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_json(data, path):\n",
    "    \n",
    "    jsonString = json.dumps(data)\n",
    "    jsonFile = open(path, \"w\")\n",
    "    jsonFile.write(jsonString)\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, predictions, labels, file_name):\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # store all article\n",
    "    predicted = []\n",
    "    true_label = []\n",
    "    \n",
    "    for k in range(len(decoded_labels)):\n",
    "        pred = decoded_preds[k]\n",
    "        label = decoded_labels[k]\n",
    "\n",
    "        pred_list = pred.split(', ')\n",
    "        label_list = label.split(', ')\n",
    "        \n",
    "        pred_list[0] = pred_list[0].split('are ')[-1]\n",
    "        label_list[0] = label_list[0].split('are ')[-1]\n",
    "\n",
    "        predicted.append(pred_list)\n",
    "        true_label.append(label_list)\n",
    "    \n",
    "    \n",
    "    # evaluation metrics\n",
    "    for idx in range(len(true_label)):\n",
    "        distractors = predicted[idx]\n",
    "        labels = true_label[idx]\n",
    "        \n",
    "        data[idx]['pred_distractors'] = distractors\n",
    "\n",
    "        act_set = set(labels)\n",
    "        pred1_set = set(distractors[:1])\n",
    "        pred3_set = set(distractors[:3])\n",
    "\n",
    "        p_1 = len(act_set & pred1_set) / float(1)\n",
    "        p_3 = len(act_set & pred3_set) / float(3)\n",
    "        r_3 = len(act_set & pred3_set) / float(len(act_set))\n",
    "\n",
    "        if p_3 == 0 and r_3 == 0:\n",
    "            f1_3 = 0\n",
    "        else:\n",
    "            f1_3 = 2 * (p_3 * r_3 / (p_3 + r_3))\n",
    "            \n",
    "        data[idx]['metric'] = {'P@1': p_1, 'P@3': p_3, 'R@3': r_3, 'F1@3': f1_3}\n",
    "        \n",
    "    write_json(data, file_name)\n",
    "    print(file_name + ' is saved :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/CTG/test_result/sciq_test_t5_text2text_pretrain_on_mcq_all_passage_level_8000.json is saved :)\n"
     ]
    }
   ],
   "source": [
    "save_data(test, predictions, labels, '/user_data/CTG/test_result/sciq_test_t5_text2text_pretrain_on_mcq_all_passage_level_5000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/added_tokens.json. We won't load it.\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/spiece.model\n",
      "loading file None\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/special_tokens_map.json\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/tokenizer_config.json\n",
      "loading configuration file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n",
      "loading weights file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-dtt-retrieve-8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=50,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"P@1\",\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 64\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 01:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.8240611553192139,\n",
       " 'test_P@1': 0.185,\n",
       " 'test_P@3': 0.11733333333333297,\n",
       " 'test_R@3': 0.11783333333333297,\n",
       " 'test_F1@3': 0.11753333333333298,\n",
       " 'test_runtime': 25.0027,\n",
       " 'test_samples_per_second': 39.996,\n",
       " 'test_steps_per_second': 0.32}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(valid_dataset)\n",
    "print('valid: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.0030803680419922,\n",
       " 'test_P@1': 0.198,\n",
       " 'test_P@3': 0.1173333333333331,\n",
       " 'test_R@3': 0.11745238095238072,\n",
       " 'test_F1@3': 0.11726666666666644,\n",
       " 'test_runtime': 27.1839,\n",
       " 'test_samples_per_second': 36.786,\n",
       " 'test_steps_per_second': 0.294}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions, test_labels, test_metrics = trainer.predict(test_dataset)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_data('/user_data/CTG/test_result/sciq_test_t5_text2text_pretrain_on_sciq_training_set_passage_level_dtt_retrieve_8000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Compounds that are capable of accepting electrons, such as o 2 or f2, are called what?\n",
      "answer: oxidants\n",
      "distractors: ['antioxidants', 'Oxygen', 'residues']\n",
      "predict: ['oxides', 'particles', 'anions']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Which type of tree is dominant in temperate forests?\n",
      "answer: deciduous\n",
      "distractors: ['vines', 'fungus', 'shrubs']\n",
      "predict: ['perennial', 'fibrous', 'coniferous']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Only about one percent of plants have lost what ability, turning them into consumers and even predators, instead of producers?\n",
      "answer: photosynthesis\n",
      "distractors: ['flowering', 'rooting', 'growth']\n",
      "predict: ['glycolysis', 'atherosclerosis', 'absorption']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Presence of a cell wall, large central vacuole, and organelles called plastids distinguish what type of cell?\n",
      "answer: plant\n",
      "distractors: ['animal', 'reproductive', 'heterotroph']\n",
      "predict: ['bird', 'animal', 'insect']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.3333333333333333, 'R@3': 0.3333333333333333, 'F1@3': 0.3333333333333333}\n",
      "\n",
      "question: Digestion of proteins begins with acids in what organ?\n",
      "answer: stomach\n",
      "distractors: ['colon', 'liver', 'brain']\n",
      "predict: ['kidneys', 'skin', 'respiratory']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: In what type of animals may a body cavity be present or absent?\n",
      "answer: triploblastic\n",
      "distractors: ['vertebrate', 'bicellular', 'nonvascular']\n",
      "predict: ['spongin', 'collagen', 'scleroprotein']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Rupture can cause fragments of what to travel via the bloodstream and become lodged in other arteries?\n",
      "answer: plaque\n",
      "distractors: ['enamel', 'red blood cells', 'white blood cells']\n",
      "predict: ['triglycerides', 'fat', 'marble']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What record shows that dinosaurs originated 200-250 million years ago?\n",
      "answer: fossil record\n",
      "distractors: ['ancient record', 'biological record', 'species record']\n",
      "predict: ['fossil magnitude', 'coal record', 'fuel cycle']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Ibuprofen and albuterol are examples of drugs whose _________ have different effects.\n",
      "answer: enantiomers\n",
      "distractors: ['nanoparticles', 'misnomers', 'analogous']\n",
      "predict: ['amines', 'ketones', 'acids']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What is another name for the vertebral column?\n",
      "answer: backbone\n",
      "distractors: ['nerve column', 'pillar', 'brain stem']\n",
      "predict: ['rib cage', 'umbilical cord', 'brain stem']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.3333333333333333, 'R@3': 0.3333333333333333, 'F1@3': 0.3333333333333333}\n",
      "\n",
      "question: When equal amounts of a strong acid such as hydrochloric acid are mixed with a strong base such as sodium hydroxide, the result is what kind of solution?\n",
      "answer: a neutral one\n",
      "distractors: ['a economical one', 'a lateral one', 'a thermodynamic one']\n",
      "predict: ['a neutral one', 'a positive one', 'a neutral']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What is required to move or change matter from one state to another?\n",
      "answer: energy\n",
      "distractors: ['food', 'gravity', 'evolution']\n",
      "predict: ['fuel', 'hydrogen', 'power']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What unit of measurement is defined to be the number of atoms in 12g of carbon-12?\n",
      "answer: one mole\n",
      "distractors: ['one quark', 'one joule', 'one ohm']\n",
      "predict: ['two mole', 'four electrons', 'one nucleus']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Movements in the mantle cause the plates to move over time in a process called what?\n",
      "answer: continental drift\n",
      "distractors: ['continental expansion', 'continental shift', 'boundary drift']\n",
      "predict: ['continental drift', 'continental stratosphere', 'continental solution']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: With wavelengths from 400-700 nm, what kind of light represents only a very small portion of the spectrum?\n",
      "answer: visible light\n",
      "distractors: ['invisible light', 'sunlight', 'ultraviolet light']\n",
      "predict: ['ultraviolet light', 'apparent light', 'infrared light']\n",
      "metric: {'P@1': 1.0, 'P@3': 0.3333333333333333, 'R@3': 0.3333333333333333, 'F1@3': 0.3333333333333333}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 100, 7):\n",
    "    example = test[i]\n",
    "    sentence = example['question']\n",
    "    answer = example['correct_answer']\n",
    "    distractors = [example['distractor1'], example['distractor2'], example['distractor3']]\n",
    "    pred_distractors = example['pred_distractors']\n",
    "    metric = example['metric']\n",
    "    \n",
    "    print('question:', sentence.replace('**blank**', '_'))\n",
    "    print('answer:', answer)\n",
    "    print('distractors:', distractors)\n",
    "    print('predict:', pred_distractors)\n",
    "    print('metric:', metric)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
