{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bart-DMLM(train-sciq-passage-level) Text2Text Generation on Sciq\n",
    "使用 Sciq dataset訓練 Bart Distractor Generation<br>\n",
    "直接使用 trainer 訓練 <br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 22 10:12:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX    Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 40%   51C    P0    81W / 280W |      0MiB / 24217MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX    Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 36%   51C    P0    65W / 280W |      0MiB / 24220MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"test on sciq with Bart\"\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = project_name\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1, 0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(item):\n",
    "    path = '../../../../data/Sciq/sciq_{}.json'.format(item)\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data('train')\n",
    "valid = read_data('valid')\n",
    "test = read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(train)\n",
    "test = list(test)\n",
    "valid = list(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11679, 1000, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What type of organism is commonly used in preparation of foods such as cheese and yogurt?',\n",
       " 'distractor3': 'viruses',\n",
       " 'distractor1': 'protozoa',\n",
       " 'distractor2': 'gymnosperms',\n",
       " 'correct_answer': 'mesophilic organisms',\n",
       " 'support': 'Mesophiles grow best in moderate temperature, typically between 25°C and 40°C (77°F and 104°F). Mesophiles are often found living in or on the bodies of humans or other animals. The optimal growth temperature of many pathogenic mesophiles is 37°C (98°F), the normal human body temperature. Mesophilic organisms have important uses in food preparation, including cheese, yogurt, beer and wine.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(data):\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    answers = []\n",
    "    for d in data:\n",
    "        sentence = d['question']\n",
    "        distractors = [d['distractor1'], d['distractor2'], d['distractor3']]\n",
    "        answer = d['correct_answer']\n",
    "        \n",
    "        # 避免dataset的label有空白\n",
    "        distractors = [dis.strip() for dis in distractors]\n",
    "        \n",
    "        sentences.append(sentence)\n",
    "        labels.append('_ of distractors are ' + ', '.join(distractors))\n",
    "        answers.append(answer)\n",
    "        \n",
    "    return sentences, answers, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent, train_answer, train_label = processData(train)\n",
    "valid_sent, valid_answer, valid_label = processData(valid)\n",
    "test_sent, test_answer, test_label = processData(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ of distractors are antioxidants, Oxygen, residues\n"
     ]
    }
   ],
   "source": [
    "print(test_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ of distractors are invisible light, sunlight, ultraviolet light\n"
     ]
    }
   ],
   "source": [
    "for l in test_label:\n",
    "    if 'ultraviolet light' in l:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11679, 11679, 11679)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sent), len(train_answer), len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n",
      "mesophilic organisms\n",
      "_ of distractors are protozoa, gymnosperms, viruses\n",
      "\n",
      "What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n",
      "coriolis effect\n",
      "_ of distractors are muon effect, centrifugal effect, tropical effect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(2):\n",
    "    print(train_sent[idx])\n",
    "    print(train_answer[idx])\n",
    "    print(train_label[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_sent, train_answer,truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_sent, valid_answer,truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_sent, test_answer,truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2264, 1907, 9, 33993, 16, 10266, 341, 11, 7094, 9, 6592, 215, 25, 7134, 8, 24351, 116, 2, 2, 12579, 6673, 22586, 28340, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>What type of organism is commonly used in preparation of foods such as cheese and yogurt?</s></s>mesophilic organisms</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_encodings.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels(encodings, distractors):\n",
    "    \n",
    "    distractors_encodings = tokenizer(distractors, padding=True)\n",
    "    labels = []\n",
    "    for i in range(len(distractors_encodings.input_ids)):\n",
    "        labels.append(distractors_encodings.input_ids[i])\n",
    "    \n",
    "    encodings[\"labels\"] = labels\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = add_labels(train_encodings, train_label)\n",
    "valid_encodings = add_labels(valid_encodings, valid_label)\n",
    "test_encodings = add_labels(test_encodings, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11679"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SciqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SciqDataset(train_encodings)\n",
    "valid_dataset = SciqDataset(valid_encodings)\n",
    "test_dataset = SciqDataset(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11679, 1000, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = torch.nn.ModuleDict({\n",
    "    'model': model,\n",
    "})\n",
    "checkpoint = torch.load('/user_data/Cloze/dtt_mask_lm_model/bart/sciq_train_3dtt_passage_level_12/checkpoints/epoch=01-dev_loss=0.14.ckpt')\n",
    "model_dict.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results-1\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=50,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"P@1\",\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps = 1,\n",
    "    report_to=\"wandb\" if os.getenv(\"WANDB_PROJECT\") else \"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # store all article\n",
    "    predicted = []\n",
    "    true_label = []\n",
    "    \n",
    "    for k in range(len(decoded_labels)):\n",
    "        pred = decoded_preds[k]\n",
    "        label = decoded_labels[k]\n",
    "\n",
    "        pred_list = pred.split(', ')\n",
    "        label_list = label.split(', ')\n",
    "        \n",
    "        pred_list[0] = pred_list[0].split('are ')[-1]\n",
    "        label_list[0] = label_list[0].split('are ')[-1]\n",
    "\n",
    "        predicted.append(pred_list)\n",
    "        true_label.append(label_list)\n",
    "\n",
    "    # evaluation metrics\n",
    "    p1 = 0\n",
    "    p3 = 0\n",
    "    r3 = 0\n",
    "    f3 = 0\n",
    "    for idx in range(len(true_label)):\n",
    "        distractors = predicted[idx]\n",
    "        labels = true_label[idx]\n",
    "\n",
    "        act_set = set(labels)\n",
    "        pred1_set = set(distractors[:1])\n",
    "        pred3_set = set(distractors[:3])\n",
    "\n",
    "        p_1 = len(act_set & pred1_set) / float(1)\n",
    "        p_3 = len(act_set & pred3_set) / float(3)\n",
    "        r_3 = len(act_set & pred3_set) / float(len(act_set))\n",
    "\n",
    "        if p_3 == 0 and r_3 == 0:\n",
    "            f1_3 = 0\n",
    "        else:\n",
    "            f1_3 = 2 * (p_3 * r_3 / (p_3 + r_3))\n",
    "\n",
    "        p1+=p_1\n",
    "        p3+=p_3\n",
    "        r3+=r_3\n",
    "        f3+=f1_3\n",
    "\n",
    "    avg_p1 = p1 / len(true_label)\n",
    "    avg_p3 = p3 / len(true_label)\n",
    "    avg_r3 = r3 / len(true_label)\n",
    "    avg_f3 = f3 / len(true_label)\n",
    "\n",
    "    result = {'P@1': avg_p1,\n",
    "              'P@3': avg_p3,\n",
    "              'R@3': avg_r3,\n",
    "              'F1@3': avg_f3}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 11679\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 18250\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mms0004284\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/user_data/CTG/train/DG/Sciq/Bart_sciq_train_passage_level/wandb/run-20230622_101309-3hjexplz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ms0004284/test%20on%20sciq%20with%20Bart/runs/3hjexplz\" target=\"_blank\">./results-1</a></strong> to <a href=\"https://wandb.ai/ms0004284/test%20on%20sciq%20with%20Bart\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18250' max='18250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18250/18250 2:03:30, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>P@1</th>\n",
       "      <th>P@3</th>\n",
       "      <th>R@3</th>\n",
       "      <th>F1@3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680041</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.103619</td>\n",
       "      <td>0.103644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.681409</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.114333</td>\n",
       "      <td>0.113929</td>\n",
       "      <td>0.113911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.674356</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.113667</td>\n",
       "      <td>0.113286</td>\n",
       "      <td>0.113311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.676309</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.120667</td>\n",
       "      <td>0.120262</td>\n",
       "      <td>0.120244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.680730</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.123333</td>\n",
       "      <td>0.122929</td>\n",
       "      <td>0.122911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.359800</td>\n",
       "      <td>0.686550</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.129595</td>\n",
       "      <td>0.129578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.695686</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.134667</td>\n",
       "      <td>0.134452</td>\n",
       "      <td>0.134378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.341700</td>\n",
       "      <td>0.691485</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.137667</td>\n",
       "      <td>0.137452</td>\n",
       "      <td>0.137378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.707022</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>0.137333</td>\n",
       "      <td>0.137286</td>\n",
       "      <td>0.137111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.303400</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.142667</td>\n",
       "      <td>0.142619</td>\n",
       "      <td>0.142444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.726687</td>\n",
       "      <td>0.199000</td>\n",
       "      <td>0.141667</td>\n",
       "      <td>0.141643</td>\n",
       "      <td>0.141511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.288700</td>\n",
       "      <td>0.733049</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.140333</td>\n",
       "      <td>0.140667</td>\n",
       "      <td>0.140378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.269300</td>\n",
       "      <td>0.734783</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.146333</td>\n",
       "      <td>0.146286</td>\n",
       "      <td>0.146156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.749046</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.147667</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.147444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.254700</td>\n",
       "      <td>0.761621</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.147333</td>\n",
       "      <td>0.147452</td>\n",
       "      <td>0.147222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.241000</td>\n",
       "      <td>0.768983</td>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.149333</td>\n",
       "      <td>0.149286</td>\n",
       "      <td>0.149111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.780141</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.148119</td>\n",
       "      <td>0.147889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.213900</td>\n",
       "      <td>0.793502</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.149952</td>\n",
       "      <td>0.149778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.213900</td>\n",
       "      <td>0.805188</td>\n",
       "      <td>0.201000</td>\n",
       "      <td>0.147333</td>\n",
       "      <td>0.147452</td>\n",
       "      <td>0.147222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.806481</td>\n",
       "      <td>0.202000</td>\n",
       "      <td>0.144667</td>\n",
       "      <td>0.145167</td>\n",
       "      <td>0.144822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.819037</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.148833</td>\n",
       "      <td>0.148489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.828648</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.146667</td>\n",
       "      <td>0.146786</td>\n",
       "      <td>0.146556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.180200</td>\n",
       "      <td>0.833175</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.145000</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>0.145156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.167400</td>\n",
       "      <td>0.842147</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.143667</td>\n",
       "      <td>0.143952</td>\n",
       "      <td>0.143667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.847245</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.144333</td>\n",
       "      <td>0.144619</td>\n",
       "      <td>0.144333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.852922</td>\n",
       "      <td>0.193000</td>\n",
       "      <td>0.145333</td>\n",
       "      <td>0.145619</td>\n",
       "      <td>0.145333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.151100</td>\n",
       "      <td>0.859140</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.148667</td>\n",
       "      <td>0.148786</td>\n",
       "      <td>0.148556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.866176</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.148452</td>\n",
       "      <td>0.148222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.870937</td>\n",
       "      <td>0.189000</td>\n",
       "      <td>0.150667</td>\n",
       "      <td>0.150952</td>\n",
       "      <td>0.150667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.874553</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.147667</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.147933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.128400</td>\n",
       "      <td>0.877553</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.150667</td>\n",
       "      <td>0.150952</td>\n",
       "      <td>0.150667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.886073</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.147667</td>\n",
       "      <td>0.147952</td>\n",
       "      <td>0.147667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.886353</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.148619</td>\n",
       "      <td>0.148333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.888536</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.150333</td>\n",
       "      <td>0.150619</td>\n",
       "      <td>0.150333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.111900</td>\n",
       "      <td>0.895389</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.146333</td>\n",
       "      <td>0.146619</td>\n",
       "      <td>0.146333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.897297</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.146333</td>\n",
       "      <td>0.146619</td>\n",
       "      <td>0.146333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.898188</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.150333</td>\n",
       "      <td>0.150619</td>\n",
       "      <td>0.150333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.901998</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.146333</td>\n",
       "      <td>0.146619</td>\n",
       "      <td>0.146333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.097700</td>\n",
       "      <td>0.904081</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.148452</td>\n",
       "      <td>0.148222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>0.908520</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.149667</td>\n",
       "      <td>0.149786</td>\n",
       "      <td>0.149556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>0.910332</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.151286</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.911255</td>\n",
       "      <td>0.181000</td>\n",
       "      <td>0.153667</td>\n",
       "      <td>0.153786</td>\n",
       "      <td>0.153556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>0.914699</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.149119</td>\n",
       "      <td>0.148889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.914912</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.153000</td>\n",
       "      <td>0.153119</td>\n",
       "      <td>0.152889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.916866</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.152286</td>\n",
       "      <td>0.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.918019</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.151286</td>\n",
       "      <td>0.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.918675</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.150667</td>\n",
       "      <td>0.150786</td>\n",
       "      <td>0.150556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.918449</td>\n",
       "      <td>0.173000</td>\n",
       "      <td>0.149667</td>\n",
       "      <td>0.149786</td>\n",
       "      <td>0.149556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.917994</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.150333</td>\n",
       "      <td>0.150452</td>\n",
       "      <td>0.150222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>0.918652</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150119</td>\n",
       "      <td>0.149889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-365\n",
      "Configuration saved in ./results-1/checkpoint-365/config.json\n",
      "Model weights saved in ./results-1/checkpoint-365/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-365/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-365/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6570] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-730\n",
      "Configuration saved in ./results-1/checkpoint-730/config.json\n",
      "Model weights saved in ./results-1/checkpoint-730/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-730/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-730/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-17885] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1095\n",
      "Configuration saved in ./results-1/checkpoint-1095/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1095/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1095/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1095/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-18250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1460\n",
      "Configuration saved in ./results-1/checkpoint-1460/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1460/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1460/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1460/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-365] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-1825\n",
      "Configuration saved in ./results-1/checkpoint-1825/config.json\n",
      "Model weights saved in ./results-1/checkpoint-1825/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-1825/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-1825/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-730] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2190\n",
      "Configuration saved in ./results-1/checkpoint-2190/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2190/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2190/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2190/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1095] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2555\n",
      "Configuration saved in ./results-1/checkpoint-2555/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2555/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2555/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2555/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1460] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-2920\n",
      "Configuration saved in ./results-1/checkpoint-2920/config.json\n",
      "Model weights saved in ./results-1/checkpoint-2920/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-2920/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-2920/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-1825] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-3285\n",
      "Configuration saved in ./results-1/checkpoint-3285/config.json\n",
      "Model weights saved in ./results-1/checkpoint-3285/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-3285/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-3285/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2190] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-3650\n",
      "Configuration saved in ./results-1/checkpoint-3650/config.json\n",
      "Model weights saved in ./results-1/checkpoint-3650/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-3650/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-3650/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2555] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4015\n",
      "Configuration saved in ./results-1/checkpoint-4015/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4015/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4015/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4015/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-2920] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4380\n",
      "Configuration saved in ./results-1/checkpoint-4380/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4380/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4380/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4380/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-3285] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-4745\n",
      "Configuration saved in ./results-1/checkpoint-4745/config.json\n",
      "Model weights saved in ./results-1/checkpoint-4745/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-4745/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-4745/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-3650] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5110\n",
      "Configuration saved in ./results-1/checkpoint-5110/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5110/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5110/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5110/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4015] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5475\n",
      "Configuration saved in ./results-1/checkpoint-5475/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5475/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5475/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5475/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4745] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-5840\n",
      "Configuration saved in ./results-1/checkpoint-5840/config.json\n",
      "Model weights saved in ./results-1/checkpoint-5840/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-5840/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-5840/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5110] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6205\n",
      "Configuration saved in ./results-1/checkpoint-6205/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6205/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6205/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6205/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-4380] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6570\n",
      "Configuration saved in ./results-1/checkpoint-6570/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6570/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6570/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6570/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5475] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-6935\n",
      "Configuration saved in ./results-1/checkpoint-6935/config.json\n",
      "Model weights saved in ./results-1/checkpoint-6935/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-6935/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-6935/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-5840] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-7300\n",
      "Configuration saved in ./results-1/checkpoint-7300/config.json\n",
      "Model weights saved in ./results-1/checkpoint-7300/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-7300/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-7300/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6570] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-7665\n",
      "Configuration saved in ./results-1/checkpoint-7665/config.json\n",
      "Model weights saved in ./results-1/checkpoint-7665/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-7665/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-7665/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-6935] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-8030\n",
      "Configuration saved in ./results-1/checkpoint-8030/config.json\n",
      "Model weights saved in ./results-1/checkpoint-8030/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-8030/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-8030/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-7300] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-8395\n",
      "Configuration saved in ./results-1/checkpoint-8395/config.json\n",
      "Model weights saved in ./results-1/checkpoint-8395/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-8395/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-8395/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-7665] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-8760\n",
      "Configuration saved in ./results-1/checkpoint-8760/config.json\n",
      "Model weights saved in ./results-1/checkpoint-8760/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-8760/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-8760/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-8030] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-9125\n",
      "Configuration saved in ./results-1/checkpoint-9125/config.json\n",
      "Model weights saved in ./results-1/checkpoint-9125/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-9125/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-9125/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-8395] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-9490\n",
      "Configuration saved in ./results-1/checkpoint-9490/config.json\n",
      "Model weights saved in ./results-1/checkpoint-9490/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-9490/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-9490/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-8760] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-9855\n",
      "Configuration saved in ./results-1/checkpoint-9855/config.json\n",
      "Model weights saved in ./results-1/checkpoint-9855/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-9855/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-9855/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-9125] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-10220\n",
      "Configuration saved in ./results-1/checkpoint-10220/config.json\n",
      "Model weights saved in ./results-1/checkpoint-10220/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-10220/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-10220/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-9490] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-10585\n",
      "Configuration saved in ./results-1/checkpoint-10585/config.json\n",
      "Model weights saved in ./results-1/checkpoint-10585/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-10585/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-10585/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-9855] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-10950\n",
      "Configuration saved in ./results-1/checkpoint-10950/config.json\n",
      "Model weights saved in ./results-1/checkpoint-10950/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-10950/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-10950/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-10220] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-11315\n",
      "Configuration saved in ./results-1/checkpoint-11315/config.json\n",
      "Model weights saved in ./results-1/checkpoint-11315/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-11315/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-11315/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-10585] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-11680\n",
      "Configuration saved in ./results-1/checkpoint-11680/config.json\n",
      "Model weights saved in ./results-1/checkpoint-11680/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-11680/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-11680/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-10950] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-12045\n",
      "Configuration saved in ./results-1/checkpoint-12045/config.json\n",
      "Model weights saved in ./results-1/checkpoint-12045/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-12045/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-12045/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-11315] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-12410\n",
      "Configuration saved in ./results-1/checkpoint-12410/config.json\n",
      "Model weights saved in ./results-1/checkpoint-12410/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-12410/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-12410/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-11680] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-12775\n",
      "Configuration saved in ./results-1/checkpoint-12775/config.json\n",
      "Model weights saved in ./results-1/checkpoint-12775/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-12775/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-12775/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-12045] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-13140\n",
      "Configuration saved in ./results-1/checkpoint-13140/config.json\n",
      "Model weights saved in ./results-1/checkpoint-13140/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-13140/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-13140/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-12410] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-13505\n",
      "Configuration saved in ./results-1/checkpoint-13505/config.json\n",
      "Model weights saved in ./results-1/checkpoint-13505/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-13505/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-13505/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-12775] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-13870\n",
      "Configuration saved in ./results-1/checkpoint-13870/config.json\n",
      "Model weights saved in ./results-1/checkpoint-13870/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-13870/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-13870/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-13140] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-14235\n",
      "Configuration saved in ./results-1/checkpoint-14235/config.json\n",
      "Model weights saved in ./results-1/checkpoint-14235/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-14235/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-14235/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-13505] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-14600\n",
      "Configuration saved in ./results-1/checkpoint-14600/config.json\n",
      "Model weights saved in ./results-1/checkpoint-14600/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-14600/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-14600/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-13870] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-14965\n",
      "Configuration saved in ./results-1/checkpoint-14965/config.json\n",
      "Model weights saved in ./results-1/checkpoint-14965/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-14965/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-14965/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-14235] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-15330\n",
      "Configuration saved in ./results-1/checkpoint-15330/config.json\n",
      "Model weights saved in ./results-1/checkpoint-15330/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-15330/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-15330/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-14600] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-15695\n",
      "Configuration saved in ./results-1/checkpoint-15695/config.json\n",
      "Model weights saved in ./results-1/checkpoint-15695/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-15695/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-15695/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-14965] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-16060\n",
      "Configuration saved in ./results-1/checkpoint-16060/config.json\n",
      "Model weights saved in ./results-1/checkpoint-16060/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-16060/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-16060/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-15330] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-16425\n",
      "Configuration saved in ./results-1/checkpoint-16425/config.json\n",
      "Model weights saved in ./results-1/checkpoint-16425/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-16425/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-16425/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-15695] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-16790\n",
      "Configuration saved in ./results-1/checkpoint-16790/config.json\n",
      "Model weights saved in ./results-1/checkpoint-16790/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-16790/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-16790/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-16060] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-17155\n",
      "Configuration saved in ./results-1/checkpoint-17155/config.json\n",
      "Model weights saved in ./results-1/checkpoint-17155/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-17155/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-17155/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-16425] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-17520\n",
      "Configuration saved in ./results-1/checkpoint-17520/config.json\n",
      "Model weights saved in ./results-1/checkpoint-17520/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-17520/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-17520/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-16790] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-17885\n",
      "Configuration saved in ./results-1/checkpoint-17885/config.json\n",
      "Model weights saved in ./results-1/checkpoint-17885/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-17885/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-17885/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-17155] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results-1/checkpoint-18250\n",
      "Configuration saved in ./results-1/checkpoint-18250/config.json\n",
      "Model weights saved in ./results-1/checkpoint-18250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results-1/checkpoint-18250/tokenizer_config.json\n",
      "Special tokens file saved in ./results-1/checkpoint-18250/special_tokens_map.json\n",
      "Deleting older checkpoint [results-1/checkpoint-17520] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results-1/checkpoint-6205 (score: 0.207).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18250, training_loss=0.19010735561423106, metrics={'train_runtime': 7415.7502, 'train_samples_per_second': 78.745, 'train_steps_per_second': 2.461, 'total_flos': 3.5814186809856e+16, 'train_loss': 0.19010735561423106, 'epoch': 50.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7801409363746643,\n",
       " 'eval_P@1': 0.207,\n",
       " 'eval_P@3': 0.14799999999999996,\n",
       " 'eval_R@3': 0.14811904761904762,\n",
       " 'eval_F1@3': 0.14788888888888888,\n",
       " 'eval_runtime': 16.3893,\n",
       " 'eval_samples_per_second': 61.015,\n",
       " 'eval_steps_per_second': 1.952,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /user_data/CTG/train/DG/Sciq/Bart_sciq_train_passage_level/sciq/bart-base-text2text-sciq-pretrain-on-sciq-train-passage-level-e1\n",
      "Configuration saved in /user_data/CTG/train/DG/Sciq/Bart_sciq_train_passage_level/sciq/bart-base-text2text-sciq-pretrain-on-sciq-train-passage-level-e1/config.json\n",
      "Model weights saved in /user_data/CTG/train/DG/Sciq/Bart_sciq_train_passage_level/sciq/bart-base-text2text-sciq-pretrain-on-sciq-train-passage-level-e1/pytorch_model.bin\n",
      "tokenizer config file saved in /user_data/CTG/train/DG/Sciq/Bart_sciq_train_passage_level/sciq/bart-base-text2text-sciq-pretrain-on-sciq-train-passage-level-e1/tokenizer_config.json\n",
      "Special tokens file saved in /user_data/CTG/train/DG/Sciq/Bart_sciq_train_passage_level/sciq/bart-base-text2text-sciq-pretrain-on-sciq-train-passage-level-e1/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('/user_data/CTG/train/DG/Sciq/Bart_sciq_train_passage_level/sciq/bart-base-text2text-sciq-pretrain-on-sciq-train-passage-level-e1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.7801409363746643,\n",
       " 'test_P@1': 0.207,\n",
       " 'test_P@3': 0.14799999999999996,\n",
       " 'test_R@3': 0.14811904761904762,\n",
       " 'test_F1@3': 0.14788888888888888,\n",
       " 'test_runtime': 16.1336,\n",
       " 'test_samples_per_second': 61.982,\n",
       " 'test_steps_per_second': 1.983}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(valid_dataset)\n",
    "print('valid: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.8631112575531006,\n",
       " 'test_P@1': 0.202,\n",
       " 'test_P@3': 0.1636666666666668,\n",
       " 'test_R@3': 0.16392063492063505,\n",
       " 'test_F1@3': 0.16363333333333344,\n",
       " 'test_runtime': 15.9755,\n",
       " 'test_samples_per_second': 62.596,\n",
       " 'test_steps_per_second': 2.003}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(test_dataset)\n",
    "print('test: ')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def write_json(data, path):\n",
    "    \n",
    "    jsonString = json.dumps(data)\n",
    "    jsonFile = open(path, \"w\")\n",
    "    jsonFile.write(jsonString)\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, predictions, labels, file_name):\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # store all article\n",
    "    predicted = []\n",
    "    true_label = []\n",
    "    \n",
    "    for k in range(len(decoded_labels)):\n",
    "        pred = decoded_preds[k]\n",
    "        label = decoded_labels[k]\n",
    "\n",
    "        pred_list = pred.split(', ')\n",
    "        label_list = label.split(', ')\n",
    "        \n",
    "        pred_list[0] = pred_list[0].split('are ')[-1]\n",
    "        label_list[0] = label_list[0].split('are ')[-1]\n",
    "\n",
    "        predicted.append(pred_list)\n",
    "        true_label.append(label_list)\n",
    "    \n",
    "    \n",
    "    # evaluation metrics\n",
    "    for idx in range(len(true_label)):\n",
    "        distractors = predicted[idx]\n",
    "        labels = true_label[idx]\n",
    "        \n",
    "        data[idx]['pred_distractors'] = distractors\n",
    "\n",
    "        act_set = set(labels)\n",
    "        pred1_set = set(distractors[:1])\n",
    "        pred3_set = set(distractors[:3])\n",
    "\n",
    "        p_1 = len(act_set & pred1_set) / float(1)\n",
    "        p_3 = len(act_set & pred3_set) / float(3)\n",
    "        r_3 = len(act_set & pred3_set) / float(len(act_set))\n",
    "\n",
    "        if p_3 == 0 and r_3 == 0:\n",
    "            f1_3 = 0\n",
    "        else:\n",
    "            f1_3 = 2 * (p_3 * r_3 / (p_3 + r_3))\n",
    "            \n",
    "        data[idx]['metric'] = {'P@1': p_1, 'P@3': p_3, 'R@3': r_3, 'F1@3': f1_3}\n",
    "        \n",
    "    write_json(data, file_name)\n",
    "    print(file_name + ' is saved :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user_data/CTG/test_result/sciq_test_t5_text2text_pretrain_on_sciq_training_set_passage_level.json is saved :)\n"
     ]
    }
   ],
   "source": [
    "save_data(test, predictions, labels, '/user_data/CTG/train/DG/Sciq/Bart_sciq_train_passage_level/sciq_test_retsult/sciq_test_t5_text2text_pretrain_on_sciq_train_passage_level.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level/added_tokens.json. We won't load it.\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level/spiece.model\n",
      "loading file None\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level/special_tokens_map.json\n",
      "loading file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level/tokenizer_config.json\n",
      "loading configuration file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n",
      "loading weights file /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at /user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"/user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-600000\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"/user_data/CTG/model/t5-base-text2text-sciq-pretrain-on-sciq-train-passage-level-600000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    save_strategy = \"epoch\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=50,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"P@1\",\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 64\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.207320213317871,\n",
       " 'test_P@1': 0.214,\n",
       " 'test_P@3': 0.1496666666666667,\n",
       " 'test_R@3': 0.15047619047619049,\n",
       " 'test_F1@3': 0.14993333333333336,\n",
       " 'test_runtime': 22.7105,\n",
       " 'test_samples_per_second': 44.032,\n",
       " 'test_steps_per_second': 0.352}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions, test_labels, test_metrics = trainer.predict(test_dataset)\n",
    "test_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_data(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_data('/user_data/CTG/test_result/sciq_test_t5_text2text_pretrain_on_sciq_training_set_passage_level_600000.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Compounds that are capable of accepting electrons, such as o 2 or f2, are called what?\n",
      "answer: oxidants\n",
      "distractors: ['antioxidants', 'Oxygen', 'residues']\n",
      "predict: ['oxides', 'carbonates', 'soils']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Which type of tree is dominant in temperate forests?\n",
      "answer: deciduous\n",
      "distractors: ['vines', 'fungus', 'shrubs']\n",
      "predict: ['perennial', 'conifer', 'annual']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Only about one percent of plants have lost what ability, turning them into consumers and even predators, instead of producers?\n",
      "answer: photosynthesis\n",
      "distractors: ['flowering', 'rooting', 'growth']\n",
      "predict: ['germination', 'death', 'reproduction']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Presence of a cell wall, large central vacuole, and organelles called plastids distinguish what type of cell?\n",
      "answer: plant\n",
      "distractors: ['animal', 'reproductive', 'heterotroph']\n",
      "predict: ['protista', 'fungus', 'gymnosperm']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Digestion of proteins begins with acids in what organ?\n",
      "answer: stomach\n",
      "distractors: ['colon', 'liver', 'brain']\n",
      "predict: ['kidney', 'liver', 'skin']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.3333333333333333, 'R@3': 0.3333333333333333, 'F1@3': 0.3333333333333333}\n",
      "\n",
      "question: In what type of animals may a body cavity be present or absent?\n",
      "answer: triploblastic\n",
      "distractors: ['vertebrate', 'bicellular', 'nonvascular']\n",
      "predict: ['choanocyte', 'spicule', 'interstitial']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Rupture can cause fragments of what to travel via the bloodstream and become lodged in other arteries?\n",
      "answer: plaque\n",
      "distractors: ['enamel', 'red blood cells', 'white blood cells']\n",
      "predict: ['marble', 'collagen', 'cholesterol']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What record shows that dinosaurs originated 200-250 million years ago?\n",
      "answer: fossil record\n",
      "distractors: ['ancient record', 'biological record', 'species record']\n",
      "predict: ['fossil magnitude', 'coal record', 'fuel cycle']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Ibuprofen and albuterol are examples of drugs whose _________ have different effects.\n",
      "answer: enantiomers\n",
      "distractors: ['nanoparticles', 'misnomers', 'analogous']\n",
      "predict: ['alcohols', 'monomers', 'isotopes']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What is another name for the vertebral column?\n",
      "answer: backbone\n",
      "distractors: ['nerve column', 'pillar', 'brain stem']\n",
      "predict: ['adaptation', 'eggs', 'teeth']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: When equal amounts of a strong acid such as hydrochloric acid are mixed with a strong base such as sodium hydroxide, the result is what kind of solution?\n",
      "answer: a neutral one\n",
      "distractors: ['a economical one', 'a lateral one', 'a thermodynamic one']\n",
      "predict: ['a neutral one', 'a bleaching one', 'a']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What is required to move or change matter from one state to another?\n",
      "answer: energy\n",
      "distractors: ['food', 'gravity', 'evolution']\n",
      "predict: ['fuel', 'hydrogen', 'power']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: What unit of measurement is defined to be the number of atoms in 12g of carbon-12?\n",
      "answer: one mole\n",
      "distractors: ['one quark', 'one joule', 'one ohm']\n",
      "predict: ['one atom', 'two atoms', 'four atoms']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: Movements in the mantle cause the plates to move over time in a process called what?\n",
      "answer: continental drift\n",
      "distractors: ['continental expansion', 'continental shift', 'boundary drift']\n",
      "predict: ['continental divide', 'continental stratosphere', 'continental solution']\n",
      "metric: {'P@1': 0.0, 'P@3': 0.0, 'R@3': 0.0, 'F1@3': 0}\n",
      "\n",
      "question: With wavelengths from 400-700 nm, what kind of light represents only a very small portion of the spectrum?\n",
      "answer: visible light\n",
      "distractors: ['invisible light', 'sunlight', 'ultraviolet light']\n",
      "predict: ['ultraviolet light', 'apparent light', 'infrared light']\n",
      "metric: {'P@1': 1.0, 'P@3': 0.3333333333333333, 'R@3': 0.3333333333333333, 'F1@3': 0.3333333333333333}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 100, 7):\n",
    "    example = test[i]\n",
    "    sentence = example['question']\n",
    "    answer = example['correct_answer']\n",
    "    distractors = [example['distractor1'], example['distractor2'], example['distractor3']]\n",
    "    pred_distractors = example['pred_distractors']\n",
    "    metric = example['metric']\n",
    "    \n",
    "    print('question:', sentence.replace('**blank**', '_'))\n",
    "    print('answer:', answer)\n",
    "    print('distractors:', distractors)\n",
    "    print('predict:', pred_distractors)\n",
    "    print('metric:', metric)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
